{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c84dfadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter directory name: C:\\Users\\maaan\\Desktop\\assignment1\\corpus\n",
      "enter output path dirC:\\Users\\maaan\\Desktop\\assignment1\\\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "import os\n",
    "import pathlib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# import RegexpTokenizer() method from nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import glob\n",
    "spacy.load(\"en_core_web_sm\")\n",
    "import os\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 1085058\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "termIds = {}\n",
    "termCount={}\n",
    "\n",
    "def Stemming(tokens):\n",
    "    \n",
    "  output=[]\n",
    "  st = PorterStemmer()\n",
    "  for line in tokens:\n",
    "    output.append(\" \".join([st.stem(i) for i in line.split()]))\n",
    "  return output\n",
    "\n",
    "\n",
    "stopwords = open('stoplist.txt','r')\n",
    "stops = stopwords.readlines()\n",
    "stoplist = []\n",
    "for word in stops:\n",
    "    stoplist.append(word.strip())\n",
    "\n",
    "\n",
    "def getFiles(path):\n",
    "    Files = []\n",
    "    for path1 in pathlib.Path(path).iterdir():\n",
    "        if path1.is_file():\n",
    "          Files.append(path1)\n",
    "    return Files\n",
    "\n",
    "def removeWords(list, obj):\n",
    "    tex = obj.__str__()\n",
    "    for token in obj:\n",
    "        if token.text in list:\n",
    "            tex = tex.replace(token.text,'')\n",
    "    return tex\n",
    "\n",
    "def ExtractDatafromFiles(Files):\n",
    "    termIdCounter = 0\n",
    "    for file in range(len(Files)):\n",
    "        with open(Files[file], 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.read()\n",
    "            #print(data)\n",
    "            #removing header of file\n",
    "            text = re.sub(\"<head>.*?</head>\", \"\", data, flags=re.DOTALL)\n",
    "            #removing all the HTMl tags from file\n",
    "            CleanTags = re.compile('<.*?>')\n",
    "            text = re.sub(CleanTags,'', text)\n",
    "            #converting to lower case\n",
    "            text = text.lower()\n",
    "            #removing stop words\n",
    "            obj = nlp(text)\n",
    "            text = removeWords(stoplist, obj)\n",
    "            #tokenization\n",
    "            # Create a reference variable for Class RegexpTokenizer\n",
    "            tk = RegexpTokenizer('\\s+', gaps=True)\n",
    "            # Use tokenize method\n",
    "            tokens = tk.tokenize(text)\n",
    "            #Stemming\n",
    "            StemmedList = Stemming(tokens)\n",
    "            #creating files\n",
    "            for word in StemmedList:\n",
    "                if word not in termIds:\n",
    "                    if word not in termIds.values():\n",
    "                        termIds[termIdCounter] = word\n",
    "                        termCount[termIdCounter] = 1\n",
    "                        termIdCounter = termIdCounter + 1\n",
    "                    else:\n",
    "                        key_list = list(termIds.keys())\n",
    "                        val_list = list(termIds.values())\n",
    "                        position = val_list.index(word)\n",
    "                        termCount[position] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def writeFiles(terms, docindex):\n",
    "    f = open('termids.txt', 'a')\n",
    "    f1 = open('doc_index.txt', 'a')\n",
    "    for i in range(len(termIds)):\n",
    "        f.write((i+1).__str__() + \"\\t\" + termIds[i])\n",
    "        f.write('\\n')\n",
    "        f1.write(\"1\" + \"\\t\" + termIds[i] + \"\\t\" + termCount[i].__str__())\n",
    "        f1.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dir = input('enter directory name: ')\n",
    "    files = glob.glob(dir+'\\*')\n",
    "    #print(file)\n",
    "    with open('docids.txt','a', encoding='utf-8', errors='ignore') as f1:\n",
    "        \n",
    "        for i in range(len(files)):\n",
    "            f1.write((i+1).__str__() + \"\\t\" + os.path.basename(files[i]))\n",
    "            f1.write('\\n')\n",
    "        \n",
    "    #print(files)\n",
    "    out = input('enter output path dir')\n",
    "    docid = out + \"docids2.txt\"\n",
    "    termids = out + \"terms2.txt\"\n",
    "    docindex = out + \"docindex2.txt\"\n",
    "\n",
    "    ExtractDatafromFiles(files)\n",
    "    writeFiles(termids,docindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2ed7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90dfe59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
